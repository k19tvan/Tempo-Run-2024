{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "svZL6KV5pYIc",
        "jYne1eUwpc9d",
        "wX4aoFqZplNc",
        "1K9p1JUmq3uN",
        "LLri4v0jpxSd",
        "6SZi0zUWp-xE",
        "Qb1GS7eEqWk9",
        "bShhV3NGro3s",
        "HAUPQTa4qZN9",
        "LIPCr6h0rJYl",
        "Im0YMFnwqtRW",
        "ayiLjnUashW3",
        "YVomoBefxB8l",
        "Sg_ydxm8tad1"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***TEMPORUN2024-SOURCECODE---------------------------------------------------***"
      ],
      "metadata": {
        "id": "L8JjVGlNpEnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ****How to use: Execute each cell from top to bottom-------------------------***"
      ],
      "metadata": {
        "id": "xRiqQJvewe7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***First Model-------------------------------------------------------------------***"
      ],
      "metadata": {
        "id": "ili5rmHzpSgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Download Dataset*"
      ],
      "metadata": {
        "id": "svZL6KV5pYIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown \"https://drive.google.com/file/d/1eIoqHANvHUqb0MxkBb4vEw5TmzOAoFgC/view?usp=sharing\" --fuzzy -O /content/data_train.zip"
      ],
      "metadata": {
        "id": "ihlDw9V3pSCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Unzip Dataset*"
      ],
      "metadata": {
        "id": "jYne1eUwpc9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o \"/content/data_train.zip\" -d \"/content/data_train\""
      ],
      "metadata": {
        "id": "j51cvr7EpfSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Split Train/Val*"
      ],
      "metadata": {
        "id": "wX4aoFqZplNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "files_to_move = [\n",
        "    'IMG_0007.jpg', 'IMG_0009.jpg', 'IMG_0010.jpg', 'IMG_0019.jpg',\n",
        "    'IMG_0023.jpg', 'IMG_0030.jpg', 'IMG_0041.jpg', 'IMG_0046.jpg',\n",
        "    'IMG_0053.jpg', 'IMG_0064.jpg', 'IMG_0069.jpg', 'IMG_0082.jpg',\n",
        "    'IMG_0087.jpg', 'IMG_0089.jpg', 'IMG_0090.jpg', 'IMG_0099.jpg',\n",
        "    'IMG_0100.jpg', 'IMG_0103.jpg', 'IMG_0107.jpg', 'IMG_0118.jpg',\n",
        "    'IMG_0119.jpg', 'IMG_0125.jpg', 'IMG_0133.jpg', 'IMG_0143.jpg',\n",
        "    'IMG_0144.jpg', 'IMG_0152.jpg', 'IMG_0161.jpg', 'IMG_0169.jpg',\n",
        "    'test_007.png', 'test_018.png'\n",
        "]\n",
        "\n",
        "os.makedirs('/content/data_val/images', exist_ok=True)\n",
        "os.makedirs('/content/data_val/labels', exist_ok=True)\n",
        "\n",
        "source_img_dir = Path(\"/content/data_train/images\")\n",
        "source_label_dir = Path(\"/content/data_train/labels\")\n",
        "\n",
        "dest_img_dir = Path(\"/content/data_val/images\")\n",
        "dest_label_dir = Path(\"/content/data_val/labels\")\n",
        "\n",
        "for filename in files_to_move:\n",
        "    src_img = source_img_dir / filename\n",
        "    if src_img.exists():\n",
        "        shutil.move(str(src_img), str(dest_img_dir / filename))\n",
        "\n",
        "    label_filename = Path(filename).stem + '.txt'\n",
        "    src_label = source_label_dir / label_filename\n",
        "    if src_label.exists():\n",
        "        shutil.move(str(src_label), str(dest_label_dir / label_filename))"
      ],
      "metadata": {
        "id": "-muc6qZppjwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Create .yaml file*"
      ],
      "metadata": {
        "id": "1K9p1JUmq3uN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "import os\n",
        "\n",
        "dataset_info = {\n",
        "    'train': '/content/data_train',\n",
        "    'val': '/content/data_val',\n",
        "    'nc': 1,\n",
        "    'names': ['SignBoard']\n",
        "}\n",
        "\n",
        "yamlfile_path = './dataset_signboard.yaml'\n",
        "\n",
        "with open(yamlfile_path, 'w') as file:\n",
        "    yaml.dump(dataset_info, file, default_flow_style=None)"
      ],
      "metadata": {
        "id": "retIUOJOrE0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Train Model*"
      ],
      "metadata": {
        "id": "LLri4v0jpxSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "9IUz1LmgpzxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov9c.pt')\n",
        "model.train(\n",
        "    data='/content/dataset_signboard.yaml',\n",
        "    epochs=200,\n",
        "    imgsz=800,\n",
        "    batch=8,\n",
        "    lr0=0.00296,\n",
        "    lrf=0.00733,\n",
        "    momentum=0.85306,\n",
        "    weight_decay=0.00037,\n",
        "    warmup_epochs=2.04141,\n",
        "    warmup_momentum=0.35765,\n",
        "    box=9.10677,\n",
        "    cls=0.35439,\n",
        "    dfl=1.18218,\n",
        "    hsv_h=0.01433,\n",
        "    hsv_s=0.49166,\n",
        "    hsv_v=0.40895,\n",
        "    translate=0.0969,\n",
        "    scale=0.35887,\n",
        "    fliplr=0.49189,\n",
        "    mosaic=0.63067,\n",
        "    save_period=10,\n",
        "    optimizer='SGD'\n",
        ")"
      ],
      "metadata": {
        "id": "ARJfGmB9pypi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Reset Dataset***"
      ],
      "metadata": {
        "id": "6SZi0zUWp-xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/data_train /content/data_val"
      ],
      "metadata": {
        "id": "3dVqK_4wqE-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Second Model-------------------------------------------------------------------***"
      ],
      "metadata": {
        "id": "PM0Il2zep3vc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Unzip Dataset*"
      ],
      "metadata": {
        "id": "Qb1GS7eEqWk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o \"/content/data_train.zip\" -d \"/content/data_train\""
      ],
      "metadata": {
        "id": "IfuXEGlep5OS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Split Train/Val*"
      ],
      "metadata": {
        "id": "bShhV3NGro3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "files_to_move = [\n",
        "    'IMG_0003.jpg',\n",
        "    'IMG_0009.jpg',\n",
        "    'IMG_0030.jpg',\n",
        "    'IMG_0023.jpg',\n",
        "    'IMG_0084.jpg',\n",
        "    'IMG_0002.jpg',\n",
        "    'IMG_0029.jpg',\n",
        "    'IMG_0030.jpg',\n",
        "    'IMG_0031.jpg',\n",
        "    'IMG_0032.jpg',\n",
        "    'IMG_0081.jpg',\n",
        "    'IMG_0079.jpg',\n",
        "    'IMG_0028.jpg',\n",
        "    'IMG_0031.jpg',\n",
        "    'IMG_0113.jpg',\n",
        "    'IMG_0154.jpg',\n",
        "    'IMG_0029.jpg',\n",
        "    'IMG_0016.jpg'\n",
        "]\n",
        "\n",
        "os.makedirs('/content/data_val/images', exist_ok=True)\n",
        "os.makedirs('/content/data_val/labels', exist_ok=True)\n",
        "\n",
        "source_img_dir = Path(\"/content/data_train/images\")\n",
        "source_label_dir = Path(\"/content/data_train/labels\")\n",
        "\n",
        "dest_img_dir = Path(\"/content/data_val/images\")\n",
        "dest_label_dir = Path(\"/content/data_val/labels\")\n",
        "\n",
        "for filename in files_to_move:\n",
        "    src_img = source_img_dir / filename\n",
        "    if src_img.exists():\n",
        "        shutil.move(str(src_img), str(dest_img_dir / filename))\n",
        "\n",
        "    label_filename = Path(filename).stem + '.txt'\n",
        "    src_label = source_label_dir / label_filename\n",
        "    if src_label.exists():\n",
        "        shutil.move(str(src_label), str(dest_label_dir / label_filename))\n"
      ],
      "metadata": {
        "id": "ZMsKTDTjrrlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Augmentation*"
      ],
      "metadata": {
        "id": "HAUPQTa4qZN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import albumentations as A\n",
        "from tqdm import tqdm\n",
        "from albumentations.augmentations.geometric.transforms import Affine\n",
        "import shutil\n",
        "\n",
        "transform = A.Compose([\n",
        "    A.Affine(\n",
        "        scale=(0.8, 1.2),\n",
        "        rotate=(-30, 30),\n",
        "        translate_percent=(0.1, 0.2),\n",
        "        shear=(-10, 10),\n",
        "        p=0.6\n",
        "    ),\n",
        "    A.ColorJitter(brightness=(0.2, 1.0), contrast=0.2, p=0.5),\n",
        "    A.Blur(blur_limit=3, p=0.5),\n",
        "])\n",
        "\n",
        "special_files = [\n",
        "    'IMG_0175.jpg', 'IMG_0137.jpg', 'IMG_0133.jpg', 'IMG_0124.jpg',\n",
        "    'IMG_0103.jpg', 'IMG_0087.jpg', 'IMG_0083.jpg', 'IMG_0078.jpg',\n",
        "    'IMG_0069.jpg', 'IMG_0065.jpg', 'IMG_0058.jpg', 'IMG_0001.jpg',\n",
        "    'IMG_0012.jpg', 'IMG_0018.jpg', 'IMG_0027.jpg', 'IMG_0036.jpg',\n",
        "    'IMG_0037.jpg', 'IMG_0038.jpg', 'IMG_0052.jpg', 'IMG_0058.jpg',\n",
        "    'IMG_0071.jpg', 'IMG_0072.jpg', 'IMG_0074.jpg', 'IMG_0085.jpg',\n",
        "    'IMG_0090.jpg', 'IMG_0106.jpg', 'IMG_0111.jpg', 'IMG_0118.jpg',\n",
        "    'IMG_0125.jpg', 'IMG_0132.jpg', 'IMG_0133.jpg', 'IMG_0143.jpg',\n",
        "    'IMG_0144.jpg', 'IMG_0147.jpg', 'IMG_0151.jpg', 'IMG_0152.jpg',\n",
        "    'IMG_0153.jpg', 'test_001.png', 'test_004.png', 'test_005.png'\n",
        "    'test_011.png', 'test_013.png'\n",
        "]\n",
        "\n",
        "def augment_image(image_path, label_path, target_dir, num_augmentations):\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        print(f\"Không thể đọc ảnh: {image_path}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        with open(label_path, 'r') as f:\n",
        "            label_content = f.read()\n",
        "    except:\n",
        "        print(f\"Không thể đọc label: {label_path}\")\n",
        "        return\n",
        "\n",
        "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
        "\n",
        "    for i in range(num_augmentations):\n",
        "        augmented = transform(image=image)\n",
        "        aug_image = augmented['image']\n",
        "\n",
        "        new_image_name = f\"{base_name}_aug_{i}.jpg\"\n",
        "        new_label_name = f\"{base_name}_aug_{i}.txt\"\n",
        "\n",
        "        cv2.imwrite(os.path.join(target_dir, 'images', new_image_name), aug_image)\n",
        "        with open(os.path.join(target_dir, 'labels', new_label_name), 'w') as f:\n",
        "            f.write(label_content)\n",
        "\n",
        "def process_all_images():\n",
        "    source_dir = '/content/data_train'\n",
        "    target_dir = '/content/data_train_aug'\n",
        "\n",
        "    os.makedirs(os.path.join(target_dir, 'images'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(target_dir, 'labels'), exist_ok=True)\n",
        "\n",
        "    image_files = [f for f in os.listdir(os.path.join(source_dir, 'images'))\n",
        "                   if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "    print(\"Sao chép tập gốc vào thư mục đích...\")\n",
        "    for image_file in tqdm(image_files):\n",
        "        source_image_path = os.path.join(source_dir, 'images', image_file)\n",
        "        target_image_path = os.path.join(target_dir, 'images', image_file)\n",
        "        shutil.copy(source_image_path, target_image_path)\n",
        "\n",
        "        base_name = os.path.splitext(image_file)[0]\n",
        "        source_label_path = os.path.join(source_dir, 'labels', f\"{base_name}.txt\")\n",
        "        target_label_path = os.path.join(target_dir, 'labels', f\"{base_name}.txt\")\n",
        "        if os.path.exists(source_label_path):\n",
        "            shutil.copy(source_label_path, target_label_path)\n",
        "        else:\n",
        "            print(f\"Không tìm thấy nhãn cho ảnh: {image_file}\")\n",
        "\n",
        "    print(\"Bắt đầu quá trình augmentation...\")\n",
        "    for image_file in tqdm(image_files):\n",
        "        image_path = os.path.join(source_dir, 'images', image_file)\n",
        "        base_name = os.path.splitext(image_file)[0]\n",
        "        label_path = os.path.join(source_dir, 'labels', f\"{base_name}.txt\")\n",
        "\n",
        "        if image_file in special_files:\n",
        "            num_aug = 6\n",
        "        else:\n",
        "            num_aug = 2\n",
        "\n",
        "        augment_image(image_path, label_path, target_dir, num_aug)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        process_all_images()\n",
        "        print(\"Hoàn thành augmentation!\")\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nQuá trình bị dừng bởi người dùng\")\n",
        "    except Exception as e:\n",
        "        print(f\"Có lỗi xảy ra: {str(e)}\")"
      ],
      "metadata": {
        "id": "lttNTY7-qcdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Create .yaml file*"
      ],
      "metadata": {
        "id": "LIPCr6h0rJYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "import os\n",
        "\n",
        "dataset_info = {\n",
        "    'train': '/content/data_train_aug',\n",
        "    'val': '/content/data_val',\n",
        "    'nc': 1,\n",
        "    'names': ['SignBoard']\n",
        "}\n",
        "\n",
        "yamlfile_path = './dataset_signboard.yaml'\n",
        "\n",
        "with open(yamlfile_path, 'w') as file:\n",
        "    yaml.dump(dataset_info, file, default_flow_style=None)"
      ],
      "metadata": {
        "id": "IZgmcO_arPa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Train Model*"
      ],
      "metadata": {
        "id": "Im0YMFnwqtRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "R7poxpFyqvc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "!yolo model=yolov9c data='/content/dataset_signboard.yaml' epochs=200 imgsz=800 batch=8 lr0=0.00296 lrf=0.00733 momentum=0.85306 weight_decay=0.00037 warmup_epochs=2.04141 warmup_momentum=0.35765 box=9.10677 cls=0.35439 dfl=1.18218 hsv_h=0.01433 hsv_s=0.49166 hsv_v=0.40895 translate=0.0969 scale=0.35887 fliplr=0.49189 mosaic=0.63067 save_period=10 optimizer='SGD'"
      ],
      "metadata": {
        "id": "3PcLpKolqwUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Inference***"
      ],
      "metadata": {
        "id": "xYfXk3EvsZSg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Download TestData*"
      ],
      "metadata": {
        "id": "ayiLjnUashW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown \"https://drive.google.com/file/d/1eIoqHANvHUqb0MxkBb4vEw5TmzOAoFgC/view?usp=sharing\" --fuzzy -O /content/data_test.zip"
      ],
      "metadata": {
        "id": "Vh_fMGFfsg4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Unzip TestData*"
      ],
      "metadata": {
        "id": "YVomoBefxB8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o \"/content/data_test.zip\" -d \"/content/data_test\""
      ],
      "metadata": {
        "id": "woVEpkHNxEeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Inference*"
      ],
      "metadata": {
        "id": "Sg_ydxm8tad1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ensemble_boxes"
      ],
      "metadata": {
        "id": "qtMBAAa1uVXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import numpy as np\n",
        "from ensemble_boxes import weighted_boxes_fusion\n",
        "\n",
        "rotate_images = set([f\"image_{i}\" for i in range(602, 616)])\n",
        "rotate_images.update([f\"image_{i}\" for i in [617, 619, 620, 622, 623, 624, 625, 628, 629, 630, 633, 634, 635, 638,\n",
        "                    640, 641, 644, 646, 648, 649, 650, 651, 652, 654, 655, 657, 658, 660, 663, 665, 666, 669, 670,\n",
        "                    671, 672, 673, 674, 678, 682, 683, 684, 687, 690, 691, 692, 693, 695, 698, 701, 702, 705, 707,\n",
        "                    708, 709, 713, 715, 716, 717, 718, 721, 725, 726, 727, 728, 729, 731, 732, 733, 736, 737, 739,\n",
        "                    740, 741, 742, 744, 745]])\n",
        "rotate_images.update([\"IMG_0014\", \"IMG_0050\", \"IMG_0082\"])\n",
        "\n",
        "\n",
        "input_dir = Path(\"/content/data_test\")\n",
        "rotated_dir = Path(\"/content/data_test_rotate\")\n",
        "vis_dir = Path(\"/content/visualize\")\n",
        "answer_path = Path(\"/content/answer.txt\")\n",
        "\n",
        "rotated_dir.mkdir(exist_ok=True)\n",
        "vis_dir.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"Step 1: Rotating images...\")\n",
        "for img_path in tqdm(input_dir.iterdir()):\n",
        "    if img_path.is_file():\n",
        "        output_path = rotated_dir / img_path.name\n",
        "        img = Image.open(img_path)\n",
        "        if img_path.stem in rotate_images:\n",
        "            img = img.rotate(-90, expand=True)\n",
        "        img.save(output_path)\n",
        "\n",
        "print(\"Loading models...\")\n",
        "model1 = YOLO(\"/content/run/train/weight/epoch150.pt\")\n",
        "model2 = YOLO(\"/content/run/train2/weight/epoch10.pt\")\n",
        "\n",
        "if answer_path.exists():\n",
        "    os.remove(str(answer_path))\n",
        "answer_path.touch()\n",
        "\n",
        "def convert_rotated_box(box, original_size):\n",
        "    orig_h, orig_w = original_size\n",
        "    x1, y1, x2, y2 = box\n",
        "\n",
        "    new_x1 = y1 * orig_w\n",
        "    new_y1 = (1 - x2) * orig_h\n",
        "    new_x2 = y2 * orig_w\n",
        "    new_y2 = (1 - x1) * orig_h\n",
        "\n",
        "    return [new_x1, new_y1, new_x2, new_y2]\n",
        "\n",
        "print(\"Step 2 & 3: Predicting and visualizing...\")\n",
        "for img_path in tqdm(rotated_dir.iterdir()):\n",
        "    if not img_path.is_file():\n",
        "        continue\n",
        "\n",
        "    result1 = model1([img_path])[0]\n",
        "    result2 = model2([img_path])[0]\n",
        "\n",
        "    rotated_img = Image.open(img_path)\n",
        "    rotated_w, rotated_h = rotated_img.size\n",
        "\n",
        "    original_img_path = input_dir / img_path.name\n",
        "    original_img = Image.open(original_img_path)\n",
        "    original_image = cv2.imread(str(original_img_path))\n",
        "    orig_h, orig_w = original_image.shape[:2]\n",
        "\n",
        "    boxes1, scores1, labels1 = [], [], []\n",
        "    for bbox in result1.boxes:\n",
        "        x1, y1, x2, y2 = bbox.xyxy[0].tolist()\n",
        "        conf = float(bbox.conf[0])\n",
        "        cls = int(bbox.cls[0])\n",
        "        boxes1.append([x1/rotated_w, y1/rotated_h, x2/rotated_w, y2/rotated_h])\n",
        "        scores1.append(conf)\n",
        "        labels1.append(cls)\n",
        "\n",
        "    boxes2, scores2, labels2 = [], [], []\n",
        "    for bbox in result2.boxes:\n",
        "        x1, y1, x2, y2 = bbox.xyxy[0].tolist()\n",
        "        conf = float(bbox.conf[0])\n",
        "        cls = int(bbox.cls[0])\n",
        "        boxes2.append([x1/rotated_w, y1/rotated_h, x2/rotated_w, y2/rotated_h])\n",
        "        scores2.append(conf)\n",
        "        labels2.append(cls)\n",
        "\n",
        "    boxes, scores, labels = weighted_boxes_fusion(\n",
        "        [boxes1, boxes2],\n",
        "        [scores1, scores2],\n",
        "        [labels1, labels2],\n",
        "        weights=[1, 1],\n",
        "        iou_thr=0.3,\n",
        "        skip_box_thr=0.3,\n",
        "        conf_type='box_and_model_avg'\n",
        "    )\n",
        "\n",
        "    for box, score, label in zip(boxes, scores, labels):\n",
        "        if img_path.stem in rotate_images:\n",
        "            x1, y1, x2, y2 = convert_rotated_box(box, (orig_h, orig_w))\n",
        "        else:\n",
        "            x1, y1, x2, y2 = box\n",
        "            x1, x2 = x1 * orig_w, x2 * orig_w\n",
        "            y1, y2 = y1 * orig_h, y2 * orig_h\n",
        "\n",
        "        cv2.rectangle(original_image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 255), 2)\n",
        "        cv2.putText(original_image, f'F:{score:.2f}', (int(x1), int(y2)+45),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)\n",
        "\n",
        "    cv2.imwrite(str(vis_dir / f\"vis_{img_path.name}\"), original_image)\n",
        "\n",
        "    with open(answer_path, \"a\") as f:\n",
        "        for box, score, label in zip(boxes, scores, labels):\n",
        "            if img_path.stem in rotate_images:\n",
        "                box_orig = convert_rotated_box(box, (orig_h, orig_w))\n",
        "                x1, y1, x2, y2 = box_orig\n",
        "                x1, x2 = x1/orig_w, x2/orig_w\n",
        "                y1, y2 = y1/orig_h, y2/orig_h\n",
        "            else:\n",
        "                x1, y1, x2, y2 = box\n",
        "\n",
        "            x = (x1 + x2) / 2\n",
        "            y = (y1 + y2) / 2\n",
        "            w = x2 - x1\n",
        "            h = y2 - y1\n",
        "\n",
        "            f.write(f\"{img_path.stem} {int(label)} {x} {y} {w} {h}\\n\")\n",
        "\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "_P22S-4Utd4O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}